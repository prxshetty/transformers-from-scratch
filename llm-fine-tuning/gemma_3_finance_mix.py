# -*- coding: utf-8 -*-
"""GEMMA-3-Finance-Mix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H-9kwehRrF26Vnp4B36UbCa6WlVJ98_Y
"""

import os, re
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    import torch; v = re.match(r"[0-9\.]{3,}", str(torch.__version__)).group(0)
    xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
    !pip install --no-deps unsloth
!pip install transformers==4.55.4

from unsloth import FastModel
import torch
max_seq_length = 2048
model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-270m-it",
    max_seq_length = max_seq_length,
    load_in_4bit = False,
    load_in_8bit = False,
    full_finetuning = False,
)

model = FastModel.get_peft_model(
    model,
    r = 128,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 128,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)



from datasets import load_dataset, concatenate_datasets
from unsloth.chat_templates import get_chat_template
dataset_1 = load_dataset("gbharti/finance-alpaca")
dataset_2 = load_dataset("Balaji173/finance_news_sentiment")
dataset_3 = load_dataset("winddude/reddit_finance_43_250k")
dataset_4 = load_dataset("causal-lm/finance")

print(dataset_1["train"].column_names)

def convert_alpaca_to_chat(example):
    return {
        "conversations": [
            {"role": "system", "content": "You are a helpful financial assistant."},
            {"role": "user", "content": example["instruction"]},
            {"role": "assistant", "content": example["output"]}
        ]
    }

def convert_sentiment_to_chat(example):
    return {
        "conversations": [
            {"role": "system", "content": "You are a financial sentiment analyst."},
            {"role": "user", "content": f"What is the sentiment of this financial news: {example['context']}"},
            {"role": "assistant", "content": example["target"]}
        ]
    }

def convert_reddit_to_chat(example):
    user_content = example["title"] or ""

    if example.get("selftext") and example["selftext"].strip():
        user_content += f" {example['selftext']}"

    assistant_content = example.get("body", "") or ""
    if not user_content.strip() or not assistant_content.strip():
        return None

    return {
        "conversations": [
            {"role": "system", "content": "You are a helpful financial advisor responding to questions."},
            {"role": "user", "content": user_content.strip()},
            {"role": "assistant", "content": assistant_content.strip()}
        ]
    }

dataset_1 = dataset_1.map(convert_alpaca_to_chat, num_proc=4)
dataset_1 = dataset_1.remove_columns([col for col in dataset_1["train"].column_names if col != "conversations"])

dataset_2 = dataset_2.map(convert_sentiment_to_chat, num_proc=4)
dataset_2 = dataset_2.remove_columns([col for col in dataset_2["train"].column_names if col != "conversations"])

dataset_3 = dataset_3.map(convert_reddit_to_chat, num_proc=4)
dataset_3 = dataset_3.filter(lambda x: x is not None)
dataset_3 = dataset_3.remove_columns([col for col in dataset_3["train"].column_names if col != "conversations"])

dataset_4 = dataset_4.map(convert_alpaca_to_chat, num_proc=4)
dataset_4 = dataset_4.remove_columns([col for col in dataset_4["train"].column_names if col != "conversations"])

combined_dataset = concatenate_datasets(
    [
        dataset_1["train"],
        dataset_2["train"],
        dataset_3["train"],
        dataset_4["train"],
        dataset_4["validation"],
    ]
)
dataset = combined_dataset.train_test_split(test_size=0.2)

from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "gemma3"
)
def convert_to_chatml(example):
    text = example["text"]

    return {
        "conversations": [
            {"role": "system", "content": "You are a helpful financial assistant."},
            {"role": "user", "content": "Please provide financial information or analysis."},
            {"role": "assistant", "content": text}
        ]
    }

print(f"Combined dataset size: {len(dataset['train'])} training samples, {len(dataset['test'])} test samples")

dataset['train'][100]

def formatting_prompts_func(examples):
   convos = examples["conversations"]
   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]
   return { "text" : texts}

dataset = dataset.map(formatting_prompts_func, batched=True)

dataset['train'][100]['text']

!pip3 install wandb

import wandb
import os
from google.colab import auth

auth.authenticate_user()
api_key = os.environ.get("WANDB_API_KEY")

wandb.login(key=api_key)

from trl import SFTTrainer, SFTConfig
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset['train'],
    test_dataset = dataset['test'],
    eval_dataset = None,
    args = SFTConfig(
        dataset_text_field = "text",
        per_device_train_batch_size = 8,
        gradient_accumulation_steps = 1,
        warmup_steps = 5,
        num_train_epochs = 1,
        max_steps = 100,
        learning_rate = 5e-5,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir="outputs",
        report_to = "wandb",
    ),
)

from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<start_of_turn>user\n",
    response_part = "<start_of_turn>model\n",
)

tokenizer.decode(trainer.train_dataset[100]["input_ids"])

tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100]["labels"]]).replace(tokenizer.pad_token, " ")

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

messages = [
    {'role': 'system','content':dataset['train']['conversations'][10][0]['content']},
    {"role" : 'user', 'content' : dataset['train']['conversations'][10][1]['content']}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt = True,
).removeprefix('<bos>')

from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text, return_tensors = "pt").to("cuda"),
    max_new_tokens = 125,
    temperature = 1, top_p = 0.95, top_k = 64,
    streamer = TextStreamer(tokenizer, skip_prompt = True),
)

!pip install huggingface_hub

from huggingface_hub import login
login()

#model.save_pretrained("gemma-3")
#tokenizer.save_pretrained("gemma-3")
model.push_to_hub("prxshetty/gemma-3-finance-mix")
tokenizer.push_to_hub("prxshetty/gemma-3-finance-mix")

import torch
from datasets import load_dataset
import numpy as np
from tqdm import tqdm

test_dataset = dataset["test"]

print(f"Test dataset size: {len(test_dataset)}")
print(f"Sample test example: {test_dataset[0]['text'][:200]}...")

def calculate_perplexity(model, tokenizer, test_data, max_samples=100):
    """Calculate average perplexity on test set"""
    model.eval()
    total_loss = 0
    total_tokens = 0

    if len(test_data) > max_samples:
        import random
        indices = random.sample(range(len(test_data)), max_samples)
        test_data = test_data.select(indices)

    print(f"Evaluating on {len(test_data)} test samples...")

    with torch.no_grad():
        for example in tqdm(test_data, desc="Calculating perplexity"):
            text = example["text"]


            inputs = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to("cuda")


            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss

            num_tokens = inputs["input_ids"].numel()
            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens

    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss))

    return perplexity.item(), avg_loss

base_model, base_tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-270m-it",
    max_seq_length = max_seq_length,
    load_in_4bit = False,
    load_in_8bit = False,
    full_finetuning = False,

)


finetuned_model, finetuned_tokenizer = FastModel.from_pretrained(
    model_name = "prxshetty/gemma-3-finance-mix",
    max_seq_length = max_seq_length,
    load_in_4bit = False,
    load_in_8bit = False,
    full_finetuning = False,

)

base_perplexity, base_loss = calculate_perplexity(base_model, base_tokenizer, test_dataset, max_samples=50)
print(f"Base model - Perplexity: {base_perplexity:.2f}, Loss: {base_loss:.4f}")

finetuned_perplexity, finetuned_loss = calculate_perplexity(finetuned_model, finetuned_tokenizer, test_dataset, max_samples=50)
print(f"Fine-tuned model - Perplexity: {finetuned_perplexity:.2f}, Loss: {finetuned_loss:.4f}")

print("=" * 80)

improvement = ((base_perplexity - finetuned_perplexity) / base_perplexity) * 100
print(f"Perplexity Improvement: {improvement:.2f}%")

def evaluate_on_test_examples(num_examples=5):
    import random
    test_indices = random.sample(range(len(test_dataset)), num_examples)

    for i, idx in enumerate(test_indices):
        test_example = test_dataset[idx]
        full_text = test_example["text"]
        if "<start_of_turn>user" in full_text:
            user_start = full_text.find("<start_of_turn>user") + len("<start_of_turn>user\n")
            user_end = full_text.find("<start_of_turn>model")
            if user_end != -1:
                user_prompt = full_text[user_start:user_end].strip()
                expected_response = full_text[user_end + len("<start_of_turn>model\n"):].replace("<end_of_turn>", "").strip()
            else:
                continue
        else:
            continue

        print(f"\n TEST EXAMPLE {i+1}")
        print(f"User Prompt: {user_prompt}")
        print(f"Expected Response: {expected_response[:100]}...")
        print("-" * 60)

        messages = [
            {"role": "user", "content": user_prompt}
        ]

        prompt = base_tokenizer.apply_chat_template(
            messages,
            tokenize = False,
            add_generation_prompt = True,
        ).removeprefix('<bos>')

        print("Base Model Response:")
        inputs = base_tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = base_model.generate(
                **inputs,
                max_new_tokens = 80,
                temperature = 0.7,
                do_sample = True,
                pad_token_id = base_tokenizer.eos_token_id
            )
        base_response = base_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        print(base_response[:150] + "..." if len(base_response) > 150 else base_response)

        print("\nFine-tuned Model Response:")
        inputs = finetuned_tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = finetuned_model.generate(
                **inputs,
                max_new_tokens = 80,
                temperature = 0.7,
                do_sample = True,
                pad_token_id = finetuned_tokenizer.eos_token_id
            )
        finetuned_response = finetuned_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        print(finetuned_response[:150] + "..." if len(finetuned_response) > 150 else finetuned_response)

        print("\n" + "=" * 80)

evaluate_on_test_examples(num_examples=3)

